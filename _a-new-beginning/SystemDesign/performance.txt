React:

react.memo
useMemo
useCallback
Code Splitting & Lazy Loading

Webpack:
minification  (TerserPlugin)
uglification (TerserPlugin)
treeshaking
hashed content for caching
chunking by module or vendor
css extraction to a separate file
compression using brottli (Network-Level Optimization by CompressionWebpackPlugin)


Network:
debounce and throttling
client side caching, cdn
service worker
virtualization/Windowing, pagination
Anything that reduces file size will improve network load time and reduce TTFB
eg: compression, minification, uglification, treeshaking, webp images, css sprites
web worker
compression
Code Splitting & Lazy Loading
GraphQL

Image:
Responsive Image Sizing (Server/CDN):
Concept: Never serve an image larger than it will be displayed. The server or a specialized image CDN (like Cloudinary or Myntra's internal CDN) must dynamically generate the image at the exact required dimensions (e.g., 300Ã—400 thumbnail for the list view, not the 2000Ã—3000 high-res master file).
Benefit: Drastically reduces the file size in kilobytes.

Modern Image Formats (WebP/AVIF):
Concept: Serve images in modern formats like WebP (or AVIF) instead of JPEG or PNG.
Benefit: Provides 25â€“50% better compression than JPEG at the same perceived quality.

Lazy Loading (Intersection Observer):
Concept: The browser is instructed to defer loading the image entirely until the element comes into or nears the visible viewport.
Implementation: Use the native browser attribute loading="lazy" on the <img> tag, or use a custom implementation with the Intersection Observer API combined with virtualization.
Key Distinction: Virtualization manages the React components; Lazy Loading manages the image resource request. They work together: the virtualizer renders the component, and the lazy loader ensures the image URL isn't fetched until the component is almost visible.

Placeholder Technique (Skeleton/Blur-up):
Concept: Before the image loads, display a lightweight placeholder.
Techniques:
Skeleton Screens: A gray/shimmering block that reserves the image's space, preventing layout shift (CLS).
Low-Quality Image Placeholder (LQIP) / Blur-Up: Load an extremely tiny (e.g., 2KiB), blurry thumbnail immediately, and then swap it out for the full-resolution image once the main file is downloaded.


CSS::

Keep selectors short and simple. Avoid deeply nested selectors (e.g., body > div > #main > article > p). Never use the universal selector (*).	
Use properties like transform and opacity for animations instead of properties like top, left, width, or height
Minimize CSS Size	
Load only that css that helps with above the fold by extracting css using tools and inline them in <style> tags
rest load asynchronously
You use the rel="preload" attribute to load the file asynchronously, and then you apply the styles using onload.

HTML

<link 
  rel="preload" 
  href="/styles/non-critical.css" 
  as="style" 
  onload="this.onload=null;this.rel='stylesheet'"
/>
<noscript>
  <link rel="stylesheet" href="/styles/non-critical.css" />
</noscript>


ğŸ” The Problem

Vrboâ€™s homepage had a mix of server-rendered SEO modules and client-loaded personalized modules (like â€œContinue Your Bookingâ€, â€œYour Upcoming Tripâ€, etc).

As more personalized modules were added, the site began to show layout instability (modules popping in, pushing content down) and unnecessary network/compute load (modules loading even when not viewed).

Specifically:

A returning user saw server-rendered content first, then personalized modules loading later and shifting content around.

Many modules were requested regardless of whether the user would scroll down and see them â€” wasted requests and server work.

Bots/crawlers and real users received largely the same heavy page initially â€” even when a simpler version would suffice.

âœ… The Solution

They implemented two key strategies:

Dynamic Rendering â€” Detect if the visitor is a bot/crawler vs a real user. For bots/crawlers: serve a fully rendered server-page (good for SEO). For real users: serve a lightweight initial page (just header + search box) and defer the rest.

Module Loading Strategy / Lazy-Loading with Reserved Space â€”

They reserved fixed blank spaces (â€œslotsâ€) for each module so layout jumps are eliminated.

They loaded module content only when the user is about to scroll into view. If module has data â†’ fade-in in reserved space; if no data â†’ collapse smoothly.

ğŸ“Š The Results

For real users:

Many fewer server operations, less JavaScript run at load. Faster page ready.

Smoother module appearance â€” no surprise jumps. Less wasted data/bandwidth for mobile users.

Their custom metric Primary Action Render (PAR) improved by ~20% at P50 and ~15% at P90.

For bots/crawlers/all users:

Layout shift (CLS) improved: ~80% improvement at P50 and ~50% at P90.

Input delay (FID) improved: ~60% at P50 and ~50% at P95.

Fewer unnecessary server calls (~20% fewer) and reduced CPU load (~15% less CPU).

ğŸ“ What This Means for You (Engineer-style takeaway)

Detect context (bot vs user) and tailor what you send. Heavy full-render only when needed.

Lazy-load modules â€” and reserve space ahead of time so UI doesnâ€™t jump.

Defer work for modules that arenâ€™t immediately visible â€” saves bandwidth, CPU, improves perceived speed.

Measure the right metrics (PAR, CLS, FID) and use percentiles (P50, P90) not just averages.

Small UI changes + loading strategy = big performance/wellness wins for user experience and infrastructure.


When Expedia/Vrbo say they â€œdetect context (bot vs user)â€ â€” they mean the system checks who is making the request and how itâ€™s being made, before deciding what version of the page to serve.

Hereâ€™s how that typically works ğŸ‘‡

ğŸ•µï¸ 1ï¸âƒ£ User-Agent detection

Every HTTP request comes with a User-Agent header that identifies the requester â€” e.g.:

User-Agent: Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)


So the backend can simply check:

if (userAgent.includes("bot") || userAgent.includes("crawler") || userAgent.includes("spider")) {
  // Serve full pre-rendered HTML (SEO-optimized)
} else {
  // Serve lightweight shell + lazy-loaded modules
}


âœ… This covers bots like Googlebot, Bingbot, LinkedInBot, TwitterBot, etc.
ğŸš« It avoids wasting compute by serving full personalized JS-heavy pages to crawlers that donâ€™t execute JS anyway.

âš™ï¸ 2ï¸âƒ£ Rendering pipeline split

Once detection happens, the rendering flow branches:

Bots / crawlers:

Serve a statically rendered HTML snapshot (SSR output).

Contains all content and metadata for SEO.

No heavy personalization or client-side bootstrapping.

Real users:

Serve a lightweight skeleton (header + hero section).

Lazy-load personalized modules after first paint.

Modules render progressively as the user scrolls.

ğŸ§  3ï¸âƒ£ Why this matters

Bots donâ€™t need interactivity; they just need content for indexing.
Humans need speed, responsiveness, and minimal layout shifts.

So by separating them:

Crawlers get complete static HTML â†’ better SEO

Users get minimal payloads â†’ better performance (LCP, FID, CLS)

ğŸ§© 4ï¸âƒ£ Optional refinements

IP-based checks: For known crawler IP ranges (like Googlebotâ€™s).

Headless render detection: If the browser has no JS runtime or reports navigator.webdriver = true.

Hybrid rendering: Use a service like Rendertron or Prerender.io to dynamically serve HTML snapshots to bots while users hit the real app.

ğŸ¯ TL;DR

Vrbo identifies context by:

Inspecting User-Agent headers

Routing bots â†’ server-rendered, full-content HTML

Routing humans â†’ client-rendered, modular, lazy-loaded experience

Itâ€™s smart, because theyâ€™re not serving everyone the same heavy payload â€” they serve the right version to the right audience.




